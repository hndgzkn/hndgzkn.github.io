
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/noise_alpha/plot_empirical_strong_artifacts.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_noise_alpha_plot_empirical_strong_artifacts.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_noise_alpha_plot_empirical_strong_artifacts.py:


==========================================================
Alpha CSC on empirical time-series with strong artifacts
==========================================================

This example illustrates how to learn univariate atoms on a univariate
time-serie affected by strong artifacts. The data is a single LFP channel
recorded on a rodent's striatum [1]_. Interestingly in this time-serie, the
high frequency oscillations around 80 Hz are modulated in amplitude by the
low-frequency oscillation around 3 Hz, a phenomenon known as cross-frequency
coupling (CFC).

The convolutional sparse coding (CSC) model is able to learn the prototypical
waveforms of the signal, on which we can clearly see the CFC. However, when the
CSC model is fitted on a data section with strong artifacts, the learned atoms
do not show the expected CFC waveforms. To solve this problem, another model
can be used, called alpha-CSC [2]_, which is less affected by strong artifacts
in the data.

.. [1] G. Dallérac, M. Graupner, J. Knippenberg, R. C. R. Martinez,
    T. F. Tavares, L. Tallot, N. El Massioui, A. Verschueren, S. Höhn,
    J.B. Bertolus, et al. Updating temporal expectancy of an aversive event
    engages striatal plasticity under amygdala control.
    Nature Communications, 8:13920, 2017

.. [2] Jas, M., Dupré La Tour, T., Şimşekli, U., & Gramfort, A. (2017).
    `Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional
    Sparse Coding
    <https://papers.nips.cc/paper/6710-learning-the-morphology-of-brain-signals-using-alpha-stable-convolutional-sparse-coding.pdf>`_.
    Advances in Neural Information Processing Systems (NIPS), pages 1099--1108.

.. GENERATED FROM PYTHON SOURCE LINES 32-40

.. code-block:: default


    # Authors: Tom Dupre La Tour <tom.duprelatour@telecom-paristech.fr>
    #          Mainak Jas <mainak.jas@telecom-paristech.fr>
    #          Umut Simsekli <umut.simsekli@telecom-paristech.fr>
    #          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
    #
    # License: BSD (3-clause)








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Let us first load the data sample.

.. GENERATED FROM PYTHON SOURCE LINES 42-54

.. code-block:: default


    import mne
    import numpy as np
    import matplotlib.pyplot as plt

    # sample frequency
    sfreq = 350.

    # We load the signal. It is an LFP channel recorded on a rodent's striatum.
    data = np.load('../rodent_striatum.npy')
    print(data.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (1, 630000)




.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now let us take a closer look, plotting the 100 first seconds of the signal.

.. GENERATED FROM PYTHON SOURCE LINES 56-63

.. code-block:: default


    start, stop = [0, 100]  # in seconds
    start, stop = int(start * sfreq), int(stop * sfreq)
    time = np.arange(start, stop) / sfreq
    plt.plot(time, data[0, start:stop])
    plt.show()




.. image-sg:: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_001.png
   :alt: plot empirical strong artifacts
   :srcset: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 64-70

As we can see, the data contains severe artifacts. We will thus compare three
approaches to tackle these artifacts:
  - First, we will fit a CSC model on a section not affected by artifacts.
  - Then, we will fit a CSC model on a section affected by artifacts.
  - Finally, we will fit an alpha-CSC model on a section affected by
    artifacts.

.. GENERATED FROM PYTHON SOURCE LINES 70-94

.. code-block:: default


    # Define a clean data section.
    start, stop = [100, 600]  # in seconds
    start, stop = int(start * sfreq), int(stop * sfreq)
    data_clean = data[:, start:stop].copy()

    # Define a dirty data section (same length).
    start, stop = [0, 500]  # in seconds
    start, stop = int(start * sfreq), int(stop * sfreq)
    data_dirty = data[:, start:stop].copy()

    # We also remove the slow drift, which accounts for a lot of variance.
    data_clean = mne.filter.filter_data(data_clean, sfreq, 1, None)
    data_dirty = mne.filter.filter_data(data_dirty, sfreq, 1, None)

    # To make the most of parallel computing, we split the data into trials.
    data_clean = data_clean.reshape(50, -1)
    data_dirty = data_dirty.reshape(50, -1)

    # We scale the data, since parameter alpha is scale dependant.
    scale = data_clean.std()
    data_clean /= scale
    data_dirty /= scale





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Setting up high-pass filter at 1 Hz

    FIR filter parameters
    ---------------------
    Designing a one-pass, zero-phase, non-causal highpass filter:
    - Windowed time-domain design (firwin) method
    - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation
    - Lower passband edge: 1.00
    - Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)
    - Filter length: 1155 samples (3.300 sec)

    /scratch/hgozukan/miniconda3/lib/python3.8/site-packages/mne/filter.py:182: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      cost = (np.ceil(n_x / (N - len(h) + 1).astype(np.float)) *
    Setting up high-pass filter at 1 Hz

    FIR filter parameters
    ---------------------
    Designing a one-pass, zero-phase, non-causal highpass filter:
    - Windowed time-domain design (firwin) method
    - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation
    - Lower passband edge: 1.00
    - Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)
    - Filter length: 1155 samples (3.300 sec)





.. GENERATED FROM PYTHON SOURCE LINES 95-98

This sample contains CFC between 3 Hz and 80 Hz. This phenomenon can be
described with a comodulogram, computed for instance with the `pactools
<http://pactools.github.io/>`_ Python library.

.. GENERATED FROM PYTHON SOURCE LINES 98-107

.. code-block:: default


    from pactools import Comodulogram

    comod = Comodulogram(fs=sfreq, low_fq_range=np.arange(0.2, 10.2, 0.2),
                         low_fq_width=2., method='duprelatour')
    comod.fit(data_clean)
    comod.plot()
    plt.show()




.. image-sg:: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_002.png
   :alt: plot empirical strong artifacts
   :srcset: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [                                        ] 0% | 0.00 sec | comodulogram: DAR(10, 1)     [                                        ] 2% | 0.93 sec | comodulogram: DAR(10, 1)     [.                                       ] 4% | 1.26 sec | comodulogram: DAR(10, 1)     [..                                      ] 6% | 1.59 sec | comodulogram: DAR(10, 1)     [...                                     ] 8% | 1.94 sec | comodulogram: DAR(10, 1)     [....                                    ] 10% | 2.28 sec | comodulogram: DAR(10, 1)     [....                                    ] 12% | 2.59 sec | comodulogram: DAR(10, 1)     [.....                                   ] 14% | 2.92 sec | comodulogram: DAR(10, 1)     [......                                  ] 16% | 3.30 sec | comodulogram: DAR(10, 1)     [.......                                 ] 18% | 3.63 sec | comodulogram: DAR(10, 1)     [........                                ] 20% | 3.93 sec | comodulogram: DAR(10, 1)     [........                                ] 22% | 4.27 sec | comodulogram: DAR(10, 1)     [.........                               ] 24% | 4.56 sec | comodulogram: DAR(10, 1)     [..........                              ] 26% | 4.88 sec | comodulogram: DAR(10, 1)     [...........                             ] 28% | 5.22 sec | comodulogram: DAR(10, 1)     [............                            ] 30% | 5.53 sec | comodulogram: DAR(10, 1)     [............                            ] 32% | 5.95 sec | comodulogram: DAR(10, 1)     [.............                           ] 34% | 6.29 sec | comodulogram: DAR(10, 1)     [..............                          ] 36% | 6.59 sec | comodulogram: DAR(10, 1)     [...............                         ] 38% | 6.88 sec | comodulogram: DAR(10, 1)     [................                        ] 40% | 7.19 sec | comodulogram: DAR(10, 1)     [................                        ] 42% | 7.50 sec | comodulogram: DAR(10, 1)     [.................                       ] 44% | 7.84 sec | comodulogram: DAR(10, 1)     [..................                      ] 46% | 8.18 sec | comodulogram: DAR(10, 1)     [...................                     ] 48% | 8.54 sec | comodulogram: DAR(10, 1)     [....................                    ] 50% | 8.87 sec | comodulogram: DAR(10, 1)     [....................                    ] 52% | 9.19 sec | comodulogram: DAR(10, 1)     [.....................                   ] 54% | 9.52 sec | comodulogram: DAR(10, 1)     [......................                  ] 56% | 9.84 sec | comodulogram: DAR(10, 1)     [.......................                 ] 58% | 10.14 sec | comodulogram: DAR(10, 1)     [........................                ] 60% | 10.43 sec | comodulogram: DAR(10, 1)     [........................                ] 62% | 10.76 sec | comodulogram: DAR(10, 1)     [.........................               ] 64% | 11.10 sec | comodulogram: DAR(10, 1)     [..........................              ] 66% | 11.37 sec | comodulogram: DAR(10, 1)     [...........................             ] 68% | 11.72 sec | comodulogram: DAR(10, 1)     [............................            ] 70% | 12.15 sec | comodulogram: DAR(10, 1)     [............................            ] 72% | 12.50 sec | comodulogram: DAR(10, 1)     [.............................           ] 74% | 12.84 sec | comodulogram: DAR(10, 1)     [..............................          ] 76% | 13.13 sec | comodulogram: DAR(10, 1)     [...............................         ] 78% | 13.44 sec | comodulogram: DAR(10, 1)     [................................        ] 80% | 13.76 sec | comodulogram: DAR(10, 1)     [................................        ] 82% | 14.06 sec | comodulogram: DAR(10, 1)     [.................................       ] 84% | 14.37 sec | comodulogram: DAR(10, 1)     [..................................      ] 86% | 14.69 sec | comodulogram: DAR(10, 1)     [...................................     ] 88% | 15.00 sec | comodulogram: DAR(10, 1)     [....................................    ] 90% | 15.30 sec | comodulogram: DAR(10, 1)     [....................................    ] 92% | 15.75 sec | comodulogram: DAR(10, 1)     [.....................................   ] 94% | 16.08 sec | comodulogram: DAR(10, 1)     [......................................  ] 96% | 16.39 sec | comodulogram: DAR(10, 1)     [....................................... ] 98% | 16.71 sec | comodulogram: DAR(10, 1)     [........................................] 100% | 17.02 sec | comodulogram: DAR(10, 1) 
    [........................................] 100% | 17.02 sec | comodulogram: DAR(10, 1) 



.. GENERATED FROM PYTHON SOURCE LINES 108-109

Here we define the plotting function which display the learned atoms.

.. GENERATED FROM PYTHON SOURCE LINES 109-132

.. code-block:: default



    def plot_atoms(d_hat):
        n_atoms, n_times_atom = d_hat.shape
        n_columns = min(6, n_atoms)
        n_rows = int(np.ceil(n_atoms // n_columns))
        figsize = (4 * n_columns, 3 * n_rows)
        fig, axes = plt.subplots(n_rows, n_columns, figsize=figsize, sharey=True)
        axes = axes.ravel()

        # Plot the temporal pattern of the atom
        for kk in range(n_atoms):
            ax = axes[kk]
            time = np.arange(n_times_atom) / sfreq
            ax.plot(time, d_hat[kk], color='C%d' % kk)
            ax.set_xlim(0, n_times_atom / sfreq)
            ax.set(xlabel='Time (sec)', title="Temporal pattern %d" % kk)
            ax.grid(True)

        fig.tight_layout()
        plt.show()









.. GENERATED FROM PYTHON SOURCE LINES 133-134

Then we define the common parameters of the different models.

.. GENERATED FROM PYTHON SOURCE LINES 134-155

.. code-block:: default


    common_params = dict(
        n_atoms=3,
        n_times_atom=int(sfreq * 1.0),  # 1000. ms
        reg=3.,
        solver_z='l-bfgs',
        solver_z_kwargs=dict(factr=1e9),
        solver_d_kwargs=dict(factr=1e2),
        random_state=42,
        n_jobs=5,
        verbose=1)

    # number of iterations
    n_iter = 10

    # Parameter of the alpha-stable distribution for the alpha-CSC model.
    # 0 < alpha < 2
    # A value of 2 would correspond to the Gaussian noise model, as in vanilla CSC.
    alpha = 1.2









.. GENERATED FROM PYTHON SOURCE LINES 156-158

First, we fit a CSC model on the clean data. Interestingly, we obtain
prototypical waveforms of the signal on which we can clearly see the CFC.

.. GENERATED FROM PYTHON SOURCE LINES 158-167

.. code-block:: default


    from alphacsc import learn_d_z, learn_d_z_weighted

    X = data_clean

    _, _, d_hat, z_hat, _ = learn_d_z(X, n_iter=n_iter, **common_params)

    plot_atoms(d_hat)




.. image-sg:: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_003.png
   :alt: Temporal pattern 0, Temporal pattern 1, Temporal pattern 2
   :srcset: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    V_0/10 .........




.. GENERATED FROM PYTHON SOURCE LINES 168-170

Then, if we fit a CSC model on the dirty data, the model is strongly affected
by the artifacts, and we cannot see CFC anymore in the temporal waveforms.

.. GENERATED FROM PYTHON SOURCE LINES 170-177

.. code-block:: default


    X = data_dirty

    _, _, d_hat, z_hat, _ = learn_d_z(X, n_iter=n_iter, **common_params)

    plot_atoms(d_hat)




.. image-sg:: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_004.png
   :alt: Temporal pattern 0, Temporal pattern 1, Temporal pattern 2
   :srcset: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    V_0/10 .........




.. GENERATED FROM PYTHON SOURCE LINES 178-181

Finally, If we fit an alpha-CSC model on the dirty data, the model is less
affected by the artifacts, and we are able to see CFC in the temporal
waveforms.

.. GENERATED FROM PYTHON SOURCE LINES 181-189

.. code-block:: default


    X = data_dirty

    d_hat, z_hat, tau = learn_d_z_weighted(
        X, n_iter_optim=n_iter, n_iter_global=3, n_iter_mcmc=300,
        n_burnin_mcmc=100, alpha=alpha, **common_params)

    plot_atoms(d_hat)



.. image-sg:: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_005.png
   :alt: Temporal pattern 0, Temporal pattern 1, Temporal pattern 2
   :srcset: /auto_examples/noise_alpha/images/sphx_glr_plot_empirical_strong_artifacts_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Global Iter: 0/3        V_0/10 .........
    Global Iter: 1/3        V_0/10 .........
    Global Iter: 2/3        V_0/10 .........





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 16 minutes  46.191 seconds)


.. _sphx_glr_download_auto_examples_noise_alpha_plot_empirical_strong_artifacts.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_empirical_strong_artifacts.py <plot_empirical_strong_artifacts.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_empirical_strong_artifacts.ipynb <plot_empirical_strong_artifacts.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
